Yassmine doggaz & Salwa chaouachi \- 1√©re Master BC

# **G√©n√©ration automatique de slogans publicitaires via des mod√®les de NLP : Approche comparative**

![][image1]

**Phase 1 : Probl√©matique et √©tat de l‚Äôart**

1. **Pr√©sentation de la probl√©matique**  
    La communication publicitaire repose fortement sur des slogans percutants, capables de r√©sumer l‚Äôidentit√© d‚Äôune marque en quelques mots. La cr√©ation manuelle de tels slogans est souvent co√ªteuse, subjective et chronophage. Avec l‚Äô√©mergence de l‚Äôintelligence artificielle et du traitement automatique du langage naturel (NLP), il est d√©sormais possible d‚Äôautomatiser cette t√¢che cr√©ative. Dans ce projet, nous avons cherch√© √† explorer diff√©rentes approches de g√©n√©ration automatique de slogans publicitaires √† partir d‚Äôun corpus existant. Notre objectif √©tait d‚Äô√©valuer la capacit√© de ces approches √† produire des slogans originaux, pertinents et proches du langage marketing humain.

2. **√âtat de l‚Äôart**  
    L‚Äô√©tat de l‚Äôart en g√©n√©ration automatique de texte se divise en plusieurs grandes familles de m√©thodes :

* **Mod√®les statistiques (N-grammes)**  
   Les N-grammes sont des mod√®les probabilistes simples qui capturent la cooccurrence locale des mots. Bien qu‚Äôils soient peu co√ªteux en calcul, ils manquent de coh√©rence globale.

* **Mod√®les s√©quentiels (RNN, LSTM, GRU)**  
   Ces mod√®les apprennent les d√©pendances dans des s√©quences de texte. Ils sont capables de g√©n√©rer du texte plus fluide mais souffrent parfois de probl√®mes de m√©moire √† long terme.

* **Mod√®les transformeurs (GPT-2, GPT-3.5, GPT-4)**  
   Ces mod√®les pr√© entra√Æn√©s sur de tr√®s grands corpus sont aujourd‚Äôhui les plus performants. GPT-2 permet un fine-tuning sur des donn√©es sp√©cifiques, tandis que GPT-3.5/4 peuvent √™tre utilis√©s via des prompts sans entra√Ænement suppl√©mentaire.

* **Prompt engineering**  
   Cette approche consiste √† utiliser des instructions bien formul√©es pour guider un grand mod√®le de langage (comme GPT-4) √† produire du contenu adapt√© sans avoir besoin de donn√©es d‚Äôapprentissage suppl√©mentaires.

**Sources :**

* Jurafsky, D., & Martin, J. H. (2020). Speech and Language Processing. Pearson.

* Brown, T. et al. (2020). Language Models are Few-Shot Learners. NeurIPS.

* Bengio, Y. et al. (2003). A Neural Probabilistic Language Model.

* Survey: ‚ÄúA Survey on Text Generation Techniques for Marketing Content‚Äù, 2022, arXiv.

* OpenAI documentation: [https://platform.openai.com/docs](https://platform.openai.com/docs)

**Phase 2 : Impl√©mentation et analyse des m√©thodes**

1. **Choix des m√©thodes √† utiliser**  
    √Ä partir de l‚Äô√©tat de l‚Äôart, nous avons choisi quatre m√©thodes √† impl√©menter :

* **N-grammes :** une approche classique bas√©e sur des probabilit√©s statistiques.

* **R√©seaux de neurones r√©currents (RNN)** : une approche s√©quentielle permettant de mod√©liser des d√©pendances temporelles.

* **GPT-2** : un mod√®le pr√© entra√Æn√© bas√© sur les transformateurs, offrant une g√©n√©ration de texte fluide.

* **GPT-3.5 ou GPT-4 avec prompt engineering** : une approche moderne utilisant des invit√©s bien formul√©s pour produire des slogans sans fine-tuning.

2. **Compr√©hension et analyse des m√©thodes choisies**

* **N-grammes :**  
   Principe : Nous avons utilis√© les probabilit√©s de cooccurrence des mots pour pr√©dire le mot suivant dans une s√©quence.  
   Avantages : Simplicit√© et faible besoin en donn√©es.  
   Inconv√©nients : Manque de prise en compte du contexte global.

* **R√©seaux de neurones r√©currents (RNN) :**  
   Principe : Les RNN prennent en compte l‚Äôhistorique d‚Äôune s√©quence gr√¢ce √† une m√©moire interne.  
   Avantages : Bonne gestion du contexte local.  
   Inconv√©nients : Difficile √† entra√Æner sur de longues s√©quences.

* **GPT-2 :**  
   Principe : GPT-2 est bas√© sur les transformateurs et pr√©dit chaque mot √† partir du contexte pr√©c√©dent.  
   Avantages : Excellente qualit√© de g√©n√©ration, surtout avec un fine-tuning l√©ger.  
   Inconv√©nients : Consommation √©lev√©e de ressources.

* **GPT-3.5 ou GPT-4 avec prompt engineering :**  
   Principe : Nous avons con√ßu des prompts pour guider un grand mod√®le pr√© entra√Æn√© dans la g√©n√©ration de slogans.  
   Avantages : Tr√®s bonne qualit√© de g√©n√©ration sans entra√Ænement.  
   Inconv√©nients : D√©pendance √† l‚ÄôAPI d‚ÄôOpenAI et co√ªt associ√©.

3. **Impl√©mentation des m√©thodes**  
    Nous avons impl√©ment√© chaque m√©thode en Python, en utilisant le fichier slogans.csv comme base de donn√©es. L‚Äôensemble du code source est disponible dans le fichier Python associ√© √† ce rapport.

4. **√âtude comparative et simulations**  
    Une fois les m√©thodes impl√©ment√©es, nous avons r√©alis√© des simulations pour comparer leurs performances sur les m√™mes donn√©es.

**Crit√®res de comparaison :**

* Qualit√© des slogans g√©n√©r√©s : cr√©ativit√©, pertinence, et longueur.

* Temps de calcul : dur√©e moyenne n√©cessaire pour g√©n√©rer un slogan.

* Complexit√© de l‚Äôimpl√©mentation : facilit√© de mise en ≈ìuvre et de d√©ploiement.

Voici un tableau comparatif clair des quatre m√©thodes de g√©n√©ration automatique de slogans que vous avez impl√©ment√©es :

| Crit√®re | N-grammes | RNN | GPT-2 | GPT-3.5 / GPT-4 (Prompt Engineering) |
| ----- | ----- | ----- | ----- | ----- |
| **Principe** | Mod√®le statistique bas√© sur la cooccurrence de mots | R√©seau neuronal s√©quentiel | Mod√®le Transformer pr√© entra√Æn√© | Utilisation de prompts sur mod√®le pr√© entra√Æn√© |
| **Qualit√© des slogans** | Faible : souvent r√©p√©titif ou incoh√©rent | Moyenne : syntaxe correcte, peu cr√©atif | Bonne : fluide et plus naturel | Tr√®s bonne : slogans coh√©rents et marketing |
| **Pertinence marketing** | Faible √† moyenne | Moyenne | Bonne | Excellente |
| **Cr√©ativit√©** | Tr√®s limit√©e | Moyenne | Bonne | Tr√®s bonne |
| **Prise en compte du contexte** | Tr√®s locale (2-3 mots maximum) | Moyenne (contexte court √† moyen) | Bonne (contexte long possible) | Excellente (contexte √©tendu avec instructions) |
| **Temps de calcul** | Tr√®s rapide | Moyen √† √©lev√© | √âlev√© | Tr√®s rapide (c√¥t√© utilisateur) |
| **Complexit√© d‚Äôimpl√©mentation** | Tr√®s simple | Moyenne | Complexe (besoin de ressources) | Simple (via API, sans entra√Ænement) |
| **Ressources n√©cessaires** | Tr√®s faibles | Moyennes | √âlev√©es | Faibles (externalis√©es via API) |
| **Facilit√© de personnalisation** | Faible | Moyenne (n√©cessite entra√Ænement) | Bonne (fine-tuning possible) | Tr√®s bonne (prompting flexible) |
| **Co√ªt d‚Äôutilisation** | Aucun | Aucun | Aucun (en local, si GPU dispo) | Co√ªt li√© √† l‚ÄôAPI OpenAI |

üîç Synth√®se :

* Si l‚Äôon cherche la simplicit√© et la rapidit√© ‚Üí N-grammes.

* Si l‚Äôon veut apprendre √† mod√©liser du texte en profondeur ‚Üí RNN.

* Si l‚Äôon veut un bon compromis entre qualit√© et contr√¥le ‚Üí GPT-2.

* Si l‚Äôobjectif est la performance maximale sans entra√Æner de mod√®le ‚Üí GPT-3.5 / GPT-4 avec prompt engineering.

**Conclusion**  
 √Ä travers ce travail, nous avons pu explorer quatre approches diff√©rentes pour la g√©n√©ration automatique de slogans publicitaires. Notre √©tude comparative a mis en √©vidence les forces et faiblesses de chaque m√©thode. Si les mod√®les simples comme les N-grammes sont accessibles et rapides √† mettre en ≈ìuvre, ils restent limit√©s dans la qualit√© des r√©sultats. Les RNN offrent de meilleurs r√©sultats mais n√©cessitent plus de ressources. Les mod√®les de type GPT, notamment avec l‚Äôusage du prompt engineering, se sont r√©v√©l√©s particuli√®rement efficaces pour g√©n√©rer des slogans originaux et pertinents.

Ce projet nous a permis d‚Äôapprofondir nos connaissances en NLP et d‚Äôexp√©rimenter concr√®tement diff√©rentes techniques modernes de g√©n√©ration de texte.

[image1]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAnAAAAAECAYAAAAOPwJdAAAANElEQVR4Xu3WMQ0AMAwEseAOoJIpqGYvgrzkwcshuLqnHwAAOeoPAADsZuAAAMIYOACAMAOTpGslhjl7rwAAAABJRU5ErkJggg==>